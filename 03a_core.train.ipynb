{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from vase.config import DATA_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from vase.core.models import VanillaVAE\n",
    "from vase.core.datasets.moving_mnist import MovingFashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_data = MovingFashionMNIST(DATA_PATH, transform=ToTensor(), download=True)\n",
    "loader_iter = iter(DataLoader(fashion_data, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training For VASE\n",
    "> all losses and training code for VASE (variational inference, environmental inference, latent masking, generative replay, object classification, location regresion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup\n",
    "\n",
    "TODO: add all the distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard VAE (Reconstruction + Target KL)\n",
    "The paper's \"The Minimum Length Description (MDL)\" loss is a variannt of the standard VAE ELBO loss, maximimzing the likelihood while minimizing the KL Divergence to the prior:\n",
    "\n",
    "$$\\mathcal{L}_{MDL}(\\phi, \\theta) = E_{\\mathbf{z}^s \\sim q_{\\phi}(\\dot|\\mathbf{x}^s)}[-\\log{p_{\\theta}(\\mathbf{x}|\\mathbf{z}^s, s)}] + \\gamma |KL(q_{\\phi}(\\mathbf{z}^s|\\mathbf{x}^s)||p(z)) - C|^2$$\n",
    "\n",
    "\n",
    "However, you'll notice the KL divergence term is slighly non-standard. Rather than penalizing the KLDiv at a fixed rate, the loss is the difference between the KLDiv and a dynamic target $C$, which increases over the course of training, allowing for gradually more representation capacity. This trick was taken from [Understanding disentanglement in the $\\beta$-VAE](https://arxiv.org/pdf/1804.03599.pdf)\n",
    "\n",
    "For now we'll also drop the environment super script s, just training an autoencoder on iid data:\n",
    "\n",
    "$$\\mathcal{L}_{MDL}(\\phi, \\theta) = E_{\\mathbf{z} \\sim q_{\\phi}(\\dot|\\mathbf{x})}[-\\log{p_{\\theta}(\\mathbf{x}|\\mathbf{z})}] + \\gamma |KL(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(z)) - C|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Loss\n",
    "\n",
    "We'll use Binary Cross Entropy Loss with $y$ the ground truth image $x$, and $p(y)$ the reconstructed image. In terms of log likelihood, I'm not really sure how this makes sense, but it seems to be how its done... (TODO figure this out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def reconstruction_loss(x, x_rec):\n",
    "    return F.binary_cross_entropy(x_rec, x, reduce=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Div Target\n",
    "\n",
    "Recall the definition of KL Divergence is the expected value under the reference distribution of the information ratio (or something like that):\n",
    "\n",
    "$$D_{KL}(q||p) = E_q[\\log{\\frac{q}{p}}] $$\n",
    "\n",
    "So in our case, with\n",
    "$$KL(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(z))$$\n",
    "we have \n",
    "\n",
    "$$KL(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(z)) = E_{q_{\\phi}(\\mathbf{z}|x)}[\\log{q_{\\phi}(\\mathbf{z}|\\mathbf{x})} - \\log{p(z)}]$$\n",
    "\n",
    "Note that both $q_{\\phi}(\\mathbf{z}|x))$ and $p(z)$ are diagonal gaussians. The KL divergence between diagonal gaussians can be [derived analytically](https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians), and is given by:\n",
    "\n",
    "$$ \\log{\\frac{\\sigma_2}{\\sigma_1}} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}$$\n",
    "\n",
    "Since $p(z)$ is standard normal, we have $\\mu_2 = 0, \\sigma_2 = 1$, reducing the equation to:\n",
    "\n",
    "$$ \\frac{1}{2}(\\sigma_1^2 + \\mu_1^2 - 1) - \\log{\\sigma_1^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KLDiv Standard Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def kl_div_stdnorm(mu, logvar):\n",
    "    \"\"\"Returns mean of KL Divergence across batch\"\"\"\n",
    "    return torch.mean(0.5 * (logvar.exp() + mu.pow(2) - 1) - logvar) #NOTE: this might be off, other implementations scale logvar too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert kl_div_stdnorm(torch.Tensor([0]), torch.log(torch.Tensor([1]))) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mu_1$ = 2, $\\sigma_1^2$ = 4, then we would have\n",
    "$$KL(q, p) = \\log \\frac{1}{4} + \\frac{4 + (2-0)^2}{2} - \\frac{1}{2} = 4 - \\frac{1}{2} + log{\\frac{1}{4}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert kl_div_stdnorm(torch.Tensor([2]), torch.log(torch.Tensor([4]))) == 4 - .5 + torch.log(torch.Tensor([.25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KLDiv Target Loss\n",
    "\n",
    "Now we can define the full loss:\n",
    "\n",
    "$$\\gamma |KL(q_{\\phi}(\\mathbf{z}^s|\\mathbf{x}^s)||p(z)) - C|^2$$\n",
    "\n",
    "I'm not sure if the difference is computed element wise, or by batch...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def kl_div_target(mu, logvar, C=0, gamma=1):\n",
    "    \"\"\"Returns target loss: squared difference of mean kldivergence and target C scaled by gamma\"\"\"\n",
    "    return gamma * ((kl_div_stdnorm(mu, logvar) - C).pow(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert kl_div_target(torch.Tensor([0]), torch.log(torch.Tensor([1]))) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert kl_div_target(torch.Tensor([0]), torch.log(torch.Tensor([1])), C=1) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert kl_div_target(torch.Tensor([0]), torch.log(torch.Tensor([1])), C=2, gamma=3) == 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=1\n",
    "lr=6e-4\n",
    "batch_size = 64\n",
    "latents=24\n",
    "C=0\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_vae = VanillaVAE(latents=latents)\n",
    "optimizer = torch.optim.Adam(params = vanilla_vae.parameters(), lr=lr)\n",
    "loader = DataLoader(fashion_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverdaniels-koch/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss=132.15542602539062\n",
      "epoch: 1, loss=131.6337432861328\n",
      "epoch: 2, loss=131.08164978027344\n",
      "epoch: 3, loss=130.8853302001953\n",
      "epoch: 4, loss=131.02780151367188\n",
      "epoch: 5, loss=130.64657592773438\n",
      "epoch: 6, loss=130.9297332763672\n",
      "epoch: 7, loss=130.6197052001953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/oliverdaniels-koch/NotebookProjects/lifelong_disrep/03a_core.train.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oliverdaniels-koch/NotebookProjects/lifelong_disrep/03a_core.train.ipynb#ch0000051?line=8'>9</a>\u001b[0m kl_loss \u001b[39m=\u001b[39m kl_div_target(mu, logvar, C\u001b[39m=\u001b[39mC, gamma\u001b[39m=\u001b[39mgamma)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oliverdaniels-koch/NotebookProjects/lifelong_disrep/03a_core.train.ipynb#ch0000051?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m rec_loss \u001b[39m+\u001b[39m kl_loss\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/oliverdaniels-koch/NotebookProjects/lifelong_disrep/03a_core.train.ipynb#ch0000051?line=11'>12</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oliverdaniels-koch/NotebookProjects/lifelong_disrep/03a_core.train.ipynb#ch0000051?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oliverdaniels-koch/NotebookProjects/lifelong_disrep/03a_core.train.ipynb#ch0000051?line=13'>14</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "File \u001b[0;32m~/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/oliverdaniels-koch/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/oliverdaniels-koch/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///Users/oliverdaniels-koch/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///Users/oliverdaniels-koch/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/oliverdaniels-koch/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///Users/oliverdaniels-koch/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///Users/oliverdaniels-koch/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/oliverdaniels-koch/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///Users/oliverdaniels-koch/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/oliverdaniels-koch/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/oliverdaniels-koch/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/oliverdaniels-koch/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///Users/oliverdaniels-koch/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///Users/oliverdaniels-koch/miniforge3/envs/lifelong_disrep/lib/python3.9/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for X, _y, _pos in loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        rec_X, mu, logvar = vanilla_vae(X)\n",
    "\n",
    "        rec_loss = reconstruction_loss(X, rec_X)\n",
    "        kl_loss = kl_div_target(mu, logvar, C=C, gamma=gamma)\n",
    "        loss = rec_loss + kl_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "    print(f\"epoch: {epoch}, loss={total_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('lifelong_disrep')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
