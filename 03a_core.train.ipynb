{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from vase.config import DATA_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from vase.core.models import PaperVanillaVAE, FCVAE\n",
    "from vase.core.datasets.moving_mnist import MovingFashionMNIST, CommonMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents=24\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "mnist_data = CommonMNIST(DATA_PATH, transform=ToTensor(), download=True)\n",
    "fc_vae_helper = FCVAE(latents)\n",
    "mnist_loader = DataLoader(mnist_data, batch_size=batch_size, shuffle=True)\n",
    "mnist_batch, _ = iter(mnist_loader).next()\n",
    "rec, mu, logvar = fc_vae_helper(mnist_batch)\n",
    "fashion_data = MovingFashionMNIST(DATA_PATH, transform=ToTensor(), download=True)\n",
    "loader_iter = iter(DataLoader(fashion_data, 64, shuffle=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training For VASE\n",
    "> all losses and training code for VASE (variational inference, environmental inference, latent masking, generative replay, object classification, location regresion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup\n",
    "\n",
    "TODO: add all the distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard VAE (Reconstruction + Target KL)\n",
    "The paper's \"The Minimum Length Description (MDL)\" loss is a variannt of the standard VAE ELBO loss, maximimzing the likelihood while minimizing the KL Divergence to the prior:\n",
    "\n",
    "$$\\mathcal{L}_{MDL}(\\phi, \\theta) = E_{\\mathbf{z}^s \\sim q_{\\phi}(\\dot|\\mathbf{x}^s)}[-\\log{p_{\\theta}(\\mathbf{x}|\\mathbf{z}^s, s)}] + \\gamma |KL(q_{\\phi}(\\mathbf{z}^s|\\mathbf{x}^s)||p(z)) - C|$$\n",
    "\n",
    "\n",
    "However, you'll notice the KL divergence term is slighly non-standard. Rather than penalizing the KLDiv at a fixed rate, the loss is the difference between the KLDiv and a dynamic target $C$, which increases over the course of training, allowing for gradually more representation capacity. This trick was taken from [Understanding disentanglement in the $\\beta$-VAE](https://arxiv.org/pdf/1804.03599.pdf) (Note that in keeping with that paper, I have dropped the square from the KL term)\n",
    "\n",
    "For now we'll also drop the environment super script s, just training an autoencoder on iid data:\n",
    "\n",
    "$$\\mathcal{L}_{MDL}(\\phi, \\theta) = E_{\\mathbf{z} \\sim q_{\\phi}(\\dot|\\mathbf{x})}[-\\log{p_{\\theta}(\\mathbf{x}|\\mathbf{z})}] + \\gamma |KL(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(z)) - C|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Loss\n",
    "\n",
    "We'll use Binary Cross Entropy Loss with $y$ the ground truth image $x$, and $p(y)$ the reconstructed image. In terms of log likelihood, I'm not really sure how this makes sense, but it seems to be how its done... (TODO figure this out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(rec, mnist_batch, reduction='none').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(rec, mnist_batch, reduction='none').flatten(start_dim=1).sum(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def reconstruction_loss(x, x_rec):\n",
    "    \"\"\"Returns element wise reconstruction loss across batch\"\"\"\n",
    "    return torch.mean(F.binary_cross_entropy(x_rec, x, reduction='none').flatten(start_dim=1).sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Div Target\n",
    "\n",
    "Recall the definition of KL Divergence is the expected value under the reference distribution of the information ratio (or something like that):\n",
    "\n",
    "$$D_{KL}(q||p) = E_q[\\log{\\frac{q}{p}}] $$\n",
    "\n",
    "So in our case, with\n",
    "$$KL(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(z))$$\n",
    "we have \n",
    "\n",
    "$$KL(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(z)) = E_{q_{\\phi}(\\mathbf{z}|x)}[\\log{q_{\\phi}(\\mathbf{z}|\\mathbf{x})} - \\log{p(z)}]$$\n",
    "\n",
    "Note that both $q_{\\phi}(\\mathbf{z}|x))$ and $p(z)$ are diagonal gaussians. The KL divergence between diagonal gaussians can be [derived analytically](https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians), and is given by:\n",
    "\n",
    "$$ \\log{\\frac{\\sigma_2}{\\sigma_1}} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}$$\n",
    "\n",
    "Since $p(z)$ is standard normal, we have $\\mu_2 = 0, \\sigma_2 = 1$, reducing the equation to:\n",
    "\n",
    "$$ \\frac{1}{2}(\\sigma_1^2 + \\mu_1^2 - 1) - \\log{\\sigma_1^2}$$\n",
    "\n",
    "TODO: see Kigma 2014 for real derivation - this one is off (or Kigma's is off, idk...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KLDiv Standard Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_div = -.5 * torch.sum(1 + logvar - mu.pow(2) - logvar, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert kl_div.shape == torch.Size([batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def kl_div_stdnorm(mu, logvar):\n",
    "    \"\"\"Returns element wise KL Divergence across batch\"\"\"\n",
    "    return .5 * torch.sum(1 + logvar - mu.pow(2) - logvar, dim=1)#torch.mean(0.5 * (logvar.exp() + mu.pow(2) - 1) - logvar) #NOTE: this might be off, other implementations scale logvar too #-.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert kl_div_stdnorm(mu, logvar).shape == torch.Size([batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_stdnorm(torch.Tensor([0]), torch.log(torch.Tensor([1]))) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mu_1$ = 2, $\\sigma_1^2$ = 4, then we would have\n",
    "$$KL(q, p) = \\log \\frac{1}{4} + \\frac{4 + (2-0)^2}{2} - \\frac{1}{2} = 4 - \\frac{1}{2} + log{\\frac{1}{4}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_stdnorm(torch.Tensor([2]), torch.log(torch.Tensor([4]))) == 4 - .5 + torch.log(torch.Tensor([.25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KLDiv Target Loss\n",
    "\n",
    "Now we can define the full loss:\n",
    "\n",
    "$$\\gamma |KL(q_{\\phi}(\\mathbf{z}^s|\\mathbf{x}^s)||p(z)) - C|$$\n",
    "\n",
    "I'm not sure if the difference is computed element wise, or by batch...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def kl_div_target(mu, logvar, C=0, gamma=1):\n",
    "    \"\"\"Returns target loss: squared difference of mean kldivergence and target C scaled by gamma\"\"\"\n",
    "    return gamma * torch.mean(torch.abs((kl_div_stdnorm(mu, logvar) - C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_target(torch.Tensor([0]), torch.log(torch.Tensor([1]))) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_target(torch.Tensor([0]), torch.log(torch.Tensor([1])), C=1) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_target(torch.Tensor([0]), torch.log(torch.Tensor([1])), C=2, gamma=3) == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=100\n",
    "lr=1e-3\n",
    "batch_size = 64\n",
    "latents=24\n",
    "C=0\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_vae = PaperVanillaVAE(latents=latents, device=device) #VanillaVAE(latents=latents)\n",
    "optimizer = torch.optim.Adam(params = vanilla_vae.parameters(), lr=lr)\n",
    "loader = DataLoader(fashion_data, batch_size, shuffle=True) #DataLoader(fashion_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaperVanillaVAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (linear): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (linear_mu): Linear(in_features=256, out_features=24, bias=True)\n",
       "    (linear_logvar): Linear(in_features=256, out_features=24, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (linear2): Linear(in_features=24, out_features=256, bias=True)\n",
       "    (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "    (conv4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv1): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (relu): ReLU()\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss=13141.669921875, rec_loss=11275.388671875, total_div_loss=1866.2923583984375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    total_rec_loss = 0\n",
    "    total_div_loss = 0\n",
    "    for X, _y, _pos in loader:\n",
    "        X = X.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        rec_X, mu, logvar = vanilla_vae(X)\n",
    "\n",
    "        rec_loss = reconstruction_loss(X, rec_X)\n",
    "        kl_loss = kl_div_target(mu, logvar, C=C, gamma=gamma)\n",
    "        loss = rec_loss + kl_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        total_rec_loss += rec_loss\n",
    "        total_div_loss += kl_loss\n",
    "    print(f\"epoch: {epoch}, loss={total_loss/batch_size}, rec_loss={total_rec_loss/batch_size}, total_div_loss={total_div_loss/batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    rec_img, _mu, _logvar = vanilla_vae(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0].cpu().detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rec_img[0].cpu().detach().numpy().squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('lifelong_disrep')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
