{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from vase.config import DATA_PATH, PARAM_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from vase.core.models import PaperVanillaVAE, FCVAE\n",
    "from vase.core.datasets.moving_mnist import MovingFashionMNIST, CommonMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "latents=24\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "mnist_data = CommonMNIST(DATA_PATH, transform=ToTensor(), download=True)\n",
    "fc_vae_helper = FCVAE(latents)\n",
    "mnist_loader = DataLoader(mnist_data, batch_size=batch_size, shuffle=True)\n",
    "mnist_batch, _ = iter(mnist_loader).next()\n",
    "rec, mu, logvar = fc_vae_helper(mnist_batch)\n",
    "fashion_data = MovingFashionMNIST(DATA_PATH, transform=ToTensor(), download=True)\n",
    "fashion_loader = DataLoader(fashion_data, 64, shuffle=True)\n",
    "fashion_batch, _, _ = iter(fashion_loader).next()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training For VASE\n",
    "> all losses and training code for VASE (variational inference, environmental inference, latent masking, generative replay, object classification, location regresion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup\n",
    "\n",
    "TODO: add all the distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard VAE (Reconstruction + Target KL)\n",
    "The paper's \"The Minimum Length Description (MDL)\" loss is a variannt of the standard VAE ELBO loss, maximimzing the likelihood while minimizing the KL Divergence to the prior:\n",
    "\n",
    "$$\\mathcal{L}_{MDL}(\\phi, \\theta) = E_{\\mathbf{z}^s \\sim q_{\\phi}(\\dot|\\mathbf{x}^s)}[-\\log{p_{\\theta}(\\mathbf{x}|\\mathbf{z}^s, s)}] + \\gamma |KL(q_{\\phi}(\\mathbf{z}^s|\\mathbf{x}^s)||p(z)) - C|$$\n",
    "\n",
    "\n",
    "However, you'll notice the KL divergence term is slighly non-standard. Rather than penalizing the KLDiv at a fixed rate, the loss is the difference between the KLDiv and a dynamic target $C$, which increases over the course of training, allowing for gradually more representation capacity. This trick was taken from [Understanding disentanglement in the $\\beta$-VAE](https://arxiv.org/pdf/1804.03599.pdf) (Note that in keeping with that paper, I have dropped the square from the KL term)\n",
    "\n",
    "For now we'll also drop the environment super script s, just training an autoencoder on iid data:\n",
    "\n",
    "$$\\mathcal{L}_{MDL}(\\phi, \\theta) = E_{\\mathbf{z} \\sim q_{\\phi}(\\dot|\\mathbf{x})}[-\\log{p_{\\theta}(\\mathbf{x}|\\mathbf{z})}] + \\gamma |KL(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(z)) - C|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Loss\n",
    "\n",
    "We'll use Binary Cross Entropy Loss with $y$ the ground truth image $x$, and $p(y)$ the reconstructed image. In terms of log likelihood, I'm not really sure how this makes sense, but it seems to be how its done... (TODO figure this out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(rec, mnist_batch, reduction='none').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(rec, mnist_batch, reduction='none').flatten(start_dim=1).sum(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def rec_likelihood(x, x_rec):\n",
    "    \"\"\"Returns element wise reconstruction loss across batch\"\"\"\n",
    "    return F.binary_cross_entropy(x_rec, x, reduction='none').flatten(start_dim=1).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def reconstruction_loss(x, x_rec):\n",
    "    \"\"\"Returns mean reconstruction loss across batch\"\"\"\n",
    "    return torch.mean(rec_likelihood(x, x_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Div Target\n",
    "\n",
    "Recall the definition of KL Divergence is the expected value under the reference distribution of the information ratio (or something like that):\n",
    "\n",
    "$$D_{KL}(q||p) = E_q[\\log{\\frac{q}{p}}] $$\n",
    "\n",
    "So in our case, with\n",
    "$$KL(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(z))$$\n",
    "we have \n",
    "\n",
    "$$KL(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(z)) = E_{q_{\\phi}(\\mathbf{z}|x)}[\\log{q_{\\phi}(\\mathbf{z}|\\mathbf{x})} - \\log{p(z)}]$$\n",
    "\n",
    "Note that both $q_{\\phi}(\\mathbf{z}|x))$ and $p(z)$ are diagonal gaussians. The KL divergence between diagonal gaussians can be [derived analytically](https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians), and is given by:\n",
    "\n",
    "$$ \\log{\\frac{\\sigma_2}{\\sigma_1}} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}$$\n",
    "\n",
    "Since $p(z)$ is standard normal, we have $\\mu_2 = 0, \\sigma_2 = 1$, reducing the equation to:\n",
    "\n",
    "$$ \\frac{1}{2}(\\sigma_1^2 + \\mu_1^2 - 1) - \\log{\\sigma_1^2}$$\n",
    "\n",
    "TODO: see Kigma 2014 for real derivation - this one is off (or Kigma's is off, idk...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KLDiv Standard Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_div = -.5 * torch.sum(1 + logvar - mu.pow(2) - logvar, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert kl_div.shape == torch.Size([batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def kl_div_stdnorm(mu, logvar):\n",
    "    \"\"\"Returns element wise KL Divergence across batch\"\"\"\n",
    "    return .5 * torch.sum(1 + logvar - mu.pow(2) - logvar, dim=1)#torch.mean(0.5 * (logvar.exp() + mu.pow(2) - 1) - logvar) #NOTE: this might be off, other implementations scale logvar too #-.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert kl_div_stdnorm(mu, logvar).shape == torch.Size([batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_stdnorm(torch.Tensor([0]), torch.log(torch.Tensor([1]))) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mu_1$ = 2, $\\sigma_1^2$ = 4, then we would have\n",
    "$$KL(q, p) = \\log \\frac{1}{4} + \\frac{4 + (2-0)^2}{2} - \\frac{1}{2} = 4 - \\frac{1}{2} + log{\\frac{1}{4}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_stdnorm(torch.Tensor([2]), torch.log(torch.Tensor([4]))) == 4 - .5 + torch.log(torch.Tensor([.25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KLDiv Target Loss\n",
    "\n",
    "Now we can define the full loss:\n",
    "\n",
    "$$\\gamma |KL(q_{\\phi}(\\mathbf{z}^s|\\mathbf{x}^s)||p(z)) - C|$$\n",
    "\n",
    "I'm not sure if the difference is computed element wise, or by batch...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def kl_div_target(mu, logvar, C=0, gamma=1):\n",
    "    \"\"\"Returns target loss: squared difference of mean kldivergence and target C scaled by gamma\"\"\"\n",
    "    return gamma * torch.mean(torch.abs((kl_div_stdnorm(mu, logvar) - C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_target(torch.Tensor([0]), torch.log(torch.Tensor([1]))) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_target(torch.Tensor([0]), torch.log(torch.Tensor([1])), C=1) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_target(torch.Tensor([0]), torch.log(torch.Tensor([1])), C=2, gamma=3) == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters\n",
    "The original payer uses $\\gamma=100$, but will a scaling $C$ and 24 available latents (8 of which are actually used). For now we'll use a fixed C and define 8 latents (all of which are available to the VAE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=100\n",
    "lr=1e-3\n",
    "batch_size = 64\n",
    "latents=8\n",
    "C=0\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_vae = PaperVanillaVAE(latents=latents, device=device) #VanillaVAE(latents=latents)\n",
    "optimizer = torch.optim.Adam(params = vanilla_vae.parameters(), lr=lr)\n",
    "loader = DataLoader(fashion_data, batch_size, shuffle=True) #DataLoader(fashion_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaperVanillaVAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (linear): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (linear_mu): Linear(in_features=256, out_features=8, bias=True)\n",
       "    (linear_logvar): Linear(in_features=256, out_features=8, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (linear2): Linear(in_features=8, out_features=256, bias=True)\n",
       "    (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "    (conv4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv1): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (relu): ReLU()\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train:\n",
    "    epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss=10417.2978515625, rec_loss=9863.2177734375, total_div_loss=554.0680541992188\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    total_rec_loss = 0\n",
    "    total_div_loss = 0\n",
    "    for X, _y, _pos in loader:\n",
    "        X = X.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        rec_X, mu, logvar = vanilla_vae(X)\n",
    "\n",
    "        rec_loss = reconstruction_loss(X, rec_X)\n",
    "        kl_loss = kl_div_target(mu, logvar, C=C, gamma=gamma)\n",
    "        loss = rec_loss + kl_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        total_rec_loss += rec_loss\n",
    "        total_div_loss += kl_loss\n",
    "    print(f\"epoch: {epoch}, loss={total_loss/batch_size}, rec_loss={total_rec_loss/batch_size}, total_div_loss={total_div_loss/batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaperVanillaVAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (linear): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (linear_mu): Linear(in_features=256, out_features=8, bias=True)\n",
       "    (linear_logvar): Linear(in_features=256, out_features=8, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (linear2): Linear(in_features=8, out_features=256, bias=True)\n",
       "    (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "    (conv4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv1): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (relu): ReLU()\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if train:\n",
    "    torch.save(vanilla_vae.state_dict(), os.path.join(PARAM_PATH, 'vae_fashion'))\n",
    "state_dict = torch.load(os.path.join(PARAM_PATH, 'vae_fashion'), map_location=torch.device(device))\n",
    "vanilla_vae.load_state_dict(state_dict=state_dict)\n",
    "vanilla_vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    rec_img, _mu, _logvar = vanilla_vae(fashion_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x119597670>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(fashion_batch[0].cpu().detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1193327c0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(rec_img[0].cpu().detach().numpy().squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('lifelong_disrep')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
