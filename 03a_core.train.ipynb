{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from vase.config import DATA_PATH, PARAM_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from vase.core.models import PaperVanillaVAE, FCVAE\n",
    "from vase.core.datasets.moving_mnist import MovingFashionMNIST, CommonMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "latents=24\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "mnist_data = CommonMNIST(DATA_PATH, transform=ToTensor(), download=True)\n",
    "fc_vae_helper = FCVAE(latents)\n",
    "mnist_loader = DataLoader(mnist_data, batch_size=batch_size, shuffle=True)\n",
    "mnist_batch, _ = iter(mnist_loader).next()\n",
    "rec, mu, logvar = fc_vae_helper(mnist_batch)\n",
    "fashion_data = MovingFashionMNIST(DATA_PATH, transform=ToTensor(), download=True)\n",
    "fashion_loader = DataLoader(fashion_data, 64, shuffle=True)\n",
    "fashion_batch, _, _ = iter(fashion_loader).next()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training For VASE\n",
    "> all losses and training code for VASE (variational inference, environmental inference, latent masking, generative replay, object classification, location regresion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup\n",
    "\n",
    "TODO: add all the distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard VAE (Reconstruction + Target KL)\n",
    "The paper's \"The Minimum Length Description (MDL)\" loss is a variannt of the standard VAE ELBO loss, maximimzing the likelihood while minimizing the KL Divergence to the prior:\n",
    "\n",
    "$$\\mathcal{L}_{MDL}(\\phi, \\theta) = E_{\\mathbf{z}^s \\sim q_{\\phi}(\\dot|\\mathbf{x}^s)}[-\\log{p_{\\theta}(\\mathbf{x}|\\mathbf{z}^s, s)}] + \\gamma |KL(q_{\\phi}(\\mathbf{z}^s|\\mathbf{x}^s)||p(z)) - C|$$\n",
    "\n",
    "\n",
    "However, you'll notice the KL divergence term is slighly non-standard. Rather than penalizing the KLDiv at a fixed rate, the loss is the difference between the KLDiv and a dynamic target $C$, which increases over the course of training, allowing for gradually more representation capacity. This trick was taken from [Understanding disentanglement in the $\\beta$-VAE](https://arxiv.org/pdf/1804.03599.pdf) (Note that in keeping with that paper, I have dropped the square from the KL term)\n",
    "\n",
    "For now we'll also drop the environment super script s, just training an autoencoder on iid data:\n",
    "\n",
    "$$\\mathcal{L}_{MDL}(\\phi, \\theta) = E_{\\mathbf{z} \\sim q_{\\phi}(\\dot|\\mathbf{x})}[-\\log{p_{\\theta}(\\mathbf{x}|\\mathbf{z})}] + \\gamma |KL(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(z)) - C|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Loss\n",
    "\n",
    "We'll use Binary Cross Entropy Loss with $y$ the ground truth image $x$, and $p(y)$ the reconstructed image. In terms of log likelihood, I'm not really sure how this makes sense, but it seems to be how its done... (TODO figure this out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(rec, mnist_batch, reduction='none').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(rec, mnist_batch, reduction='none').flatten(start_dim=1).sum(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def reconstruction_loss(x, x_rec):\n",
    "    \"\"\"Returns element wise reconstruction loss across batch\"\"\"\n",
    "    return torch.mean(F.binary_cross_entropy(x_rec, x, reduction='none').flatten(start_dim=1).sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Div Target\n",
    "\n",
    "Recall the definition of KL Divergence is the expected value under the reference distribution of the information ratio (or something like that):\n",
    "\n",
    "$$D_{KL}(q||p) = E_q[\\log{\\frac{q}{p}}] $$\n",
    "\n",
    "So in our case, with\n",
    "$$KL(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(z))$$\n",
    "we have \n",
    "\n",
    "$$KL(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(z)) = E_{q_{\\phi}(\\mathbf{z}|x)}[\\log{q_{\\phi}(\\mathbf{z}|\\mathbf{x})} - \\log{p(z)}]$$\n",
    "\n",
    "Note that both $q_{\\phi}(\\mathbf{z}|x))$ and $p(z)$ are diagonal gaussians. The KL divergence between diagonal gaussians can be [derived analytically](https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians), and is given by:\n",
    "\n",
    "$$ \\log{\\frac{\\sigma_2}{\\sigma_1}} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}$$\n",
    "\n",
    "Since $p(z)$ is standard normal, we have $\\mu_2 = 0, \\sigma_2 = 1$, reducing the equation to:\n",
    "\n",
    "$$ \\frac{1}{2}(\\sigma_1^2 + \\mu_1^2 - 1) - \\log{\\sigma_1^2}$$\n",
    "\n",
    "TODO: see Kigma 2014 for real derivation - this one is off (or Kigma's is off, idk...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KLDiv Standard Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_div = -.5 * torch.sum(1 + logvar - mu.pow(2) - logvar, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert kl_div.shape == torch.Size([batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def kl_div_stdnorm(mu, logvar):\n",
    "    \"\"\"Returns element wise KL Divergence across batch\"\"\"\n",
    "    return .5 * torch.sum(1 + logvar - mu.pow(2) - logvar, dim=1)#torch.mean(0.5 * (logvar.exp() + mu.pow(2) - 1) - logvar) #NOTE: this might be off, other implementations scale logvar too #-.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert kl_div_stdnorm(mu, logvar).shape == torch.Size([batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_stdnorm(torch.Tensor([0]), torch.log(torch.Tensor([1]))) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mu_1$ = 2, $\\sigma_1^2$ = 4, then we would have\n",
    "$$KL(q, p) = \\log \\frac{1}{4} + \\frac{4 + (2-0)^2}{2} - \\frac{1}{2} = 4 - \\frac{1}{2} + log{\\frac{1}{4}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_stdnorm(torch.Tensor([2]), torch.log(torch.Tensor([4]))) == 4 - .5 + torch.log(torch.Tensor([.25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KLDiv Target Loss\n",
    "\n",
    "Now we can define the full loss:\n",
    "\n",
    "$$\\gamma |KL(q_{\\phi}(\\mathbf{z}^s|\\mathbf{x}^s)||p(z)) - C|$$\n",
    "\n",
    "I'm not sure if the difference is computed element wise, or by batch...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def kl_div_target(mu, logvar, C=0, gamma=1):\n",
    "    \"\"\"Returns target loss: squared difference of mean kldivergence and target C scaled by gamma\"\"\"\n",
    "    return gamma * torch.mean(torch.abs((kl_div_stdnorm(mu, logvar) - C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_target(torch.Tensor([0]), torch.log(torch.Tensor([1]))) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_target(torch.Tensor([0]), torch.log(torch.Tensor([1])), C=1) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_target(torch.Tensor([0]), torch.log(torch.Tensor([1])), C=2, gamma=3) == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters\n",
    "The original payer uses $\\gamma=100$, but will a scaling $C$ and 24 available latents (8 of which are actually used). For now we'll use a fixed C and define 8 latents (all of which are available to the VAE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=100\n",
    "lr=1e-3\n",
    "batch_size = 64\n",
    "latents=8\n",
    "C=0\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_vae = PaperVanillaVAE(latents=latents, device=device) #VanillaVAE(latents=latents)\n",
    "optimizer = torch.optim.Adam(params = vanilla_vae.parameters(), lr=lr)\n",
    "loader = DataLoader(fashion_data, batch_size, shuffle=True) #DataLoader(fashion_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaperVanillaVAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (linear): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (linear_mu): Linear(in_features=256, out_features=8, bias=True)\n",
       "    (linear_logvar): Linear(in_features=256, out_features=8, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (linear2): Linear(in_features=8, out_features=256, bias=True)\n",
       "    (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "    (conv4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv1): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (relu): ReLU()\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train:\n",
    "    epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    total_rec_loss = 0\n",
    "    total_div_loss = 0\n",
    "    for X, _y, _pos in loader:\n",
    "        X = X.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        rec_X, mu, logvar = vanilla_vae(X)\n",
    "\n",
    "        rec_loss = reconstruction_loss(X, rec_X)\n",
    "        kl_loss = kl_div_target(mu, logvar, C=C, gamma=gamma)\n",
    "        loss = rec_loss + kl_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        total_rec_loss += rec_loss\n",
    "        total_div_loss += kl_loss\n",
    "    print(f\"epoch: {epoch}, loss={total_loss/batch_size}, rec_loss={total_rec_loss/batch_size}, total_div_loss={total_div_loss/batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaperVanillaVAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (linear): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (linear_mu): Linear(in_features=256, out_features=8, bias=True)\n",
       "    (linear_logvar): Linear(in_features=256, out_features=8, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (linear2): Linear(in_features=8, out_features=256, bias=True)\n",
       "    (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "    (conv4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv1): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (relu): ReLU()\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if train:\n",
    "    torch.save(vanilla_vae.state_dict(), os.path.join(PARAM_PATH, 'vae_fashion'))\n",
    "state_dict = torch.load(os.path.join(PARAM_PATH, 'vae_fashion'), map_location=torch.device(device))\n",
    "vanilla_vae.load_state_dict(state_dict=state_dict)\n",
    "vanilla_vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    rec_img, _mu, _logvar = vanilla_vae(fashion_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11fefd7c0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUiUlEQVR4nO3dfYwd1XnH8e+z7/YaYzsBx2UhNrEJcaPEpMtbIQmBEhGaxqilKDRprciSW4VWoCRKoFXTpGqlRK1I+IOmtQqJI6UB8kKMnFfXhUZQZGyCQwwOweGl2DE2NnZs/LLe3fv0jzuemTO9d/d675171zm/j7TaM3Pm7n3Wdx/POXNmzjF3R0R+83V1OgARaQ8lu0gklOwikVCyi0RCyS4SCSW7SCSaSnYzu8bMnjGz7WZ2a6uCEpHWs6mOs5tZN/AL4GpgB7AJuNHdn25deCLSKj1NvPYiYLu7PwdgZvcAy4G6yd5n/T7AYBNvKSITOcZhjvuI1aprJtnPAl7Kbe8ALp7oBQMMcrFd1cRbishENvqGunXNJHtDzGwVsApggJllv52I1NHMBbqdwNm57aFkX8DdV7v7sLsP99LfxNuJSDOaSfZNwBIzW2RmfcAHgQdaE5aItNqUm/HuPmZmfwn8EOgG7nb3p1oWmYi0VFN9dnf/HvC9FsUiIiXSHXQikVCyi0RCyS4SCSW7SCSU7CKRULKLRELJLhIJJbtIJJTsIpFQsotEQskuEgklu0gklOwikVCyi0RCyS4SCSW7SCSU7CKRULKLRELJLhIJJbtIJJTsIpFQsotEQskuEgklu0gklOwikZg02c3sbjPbY2Zbc/vmmdl6M3s2+T633DBFpFmNnNm/AlxT2HcrsMHdlwAbkm0RmcYmTXZ3/zHwamH3cmBNUl4DXNfasESk1abaZ5/v7ruS8svA/BbFIyIlafoCnbs74PXqzWyVmW02s82jjDT7diIyRVNN9t1mtgAg+b6n3oHuvtrdh919uJf+Kb6diDRrqsn+ALAiKa8A1rYmHBEpSyNDb18HHgXebGY7zGwl8DngajN7Fvi9ZFtEprGeyQ5w9xvrVF3V4lhEpES6g04kEkp2kUgo2UUioWQXiYSSXSQSSnaRSCjZRSKhZBeJhJJdJBJKdpFIKNlFIqFkF4mEkl0kEkp2kUgo2UUioWQXiYSSXSQSSnaRSCjZRSKhZBeJhJJdJBJKdpFIKNlFIqFkF4mEkl0kEo0s/3S2mT1oZk+b2VNmdnOyf56ZrTezZ5Pvc8sPV0SmqpEz+xjwcXdfClwC3GRmS4FbgQ3uvgTYkGyLyDQ1abK7+y53/0lSPgRsA84ClgNrksPWANeVFKOItMBJ9dnNbCFwAbARmO/uu5Kql4H5rQ1NRFqp4WQ3s1nAt4Bb3P1gvs7dHfA6r1tlZpvNbPMoI00FKyJT11Cym1kv1UT/mrt/O9m928wWJPULgD21Xuvuq9192N2He+lvRcwiMgWNXI034C5gm7vfnqt6AFiRlFcAa1sfnoi0Sk8Dx1wG/CnwMzPbkuz7a+BzwH1mthJ4EbihlAhFpCUmTXZ3fxiwOtVXtTYcESmL7qATiYSSXSQSSnaRSCjZRSKhZBeJhJJdJBJKdpFIKNlFIqFkF4mEkl0kEkp2kUgo2UUioWQXiYSSXSQSSnaRSCjZRSKhZBeJhJJdJBJKdpFIKNlFIqFkF4mEkl0kEkp2kUgo2UUioWQXiUQja70NmNljZvZTM3vKzD6b7F9kZhvNbLuZ3WtmfeWHKyJT1ciZfQS40t3fDiwDrjGzS4DPA19w98XAfmBlaVGKSNMmTXavei3Z7E2+HLgS+Gayfw1wXRkBikhrNLo+e3eyguseYD3wS+CAu48lh+wAziolQhFpiYaS3d3H3X0ZMARcBJzf6BuY2Soz22xmm0cZmVqUItK0k7oa7+4HgAeBS4E5ZnZiyechYGed16x292F3H+6lv5lYRaQJk67PbmZnAKPufsDMZgBXU7049yBwPXAPsAJYW2ag8pvPesI/Rx8fz2143dftW3lpWh55/6+DusOHBrIfcbQ7LffNCVuZ/RtnpeUFt/9PwzFi2fnSR4/Xfd10MGmyAwuANWbWTbUlcJ+7rzOzp4F7zOwfgCeAu0qMU0SaNGmyu/uTwAU19j9Htf8uIqeARs7sIq3T1R1uV7Kmuo+N0YjuM84Itu/99D+l5Tf1zgrqHjqaNbPndB1Ny71WCY4buiwr3/jla4O68f37G4vRLFcuXA7L/Z6dottlRSKhZBeJhJrxUr58032C5mz37NnB9kt//ta0POvK3Wn575asC44b6smGdL9zOGzGv6E7uzrfZdkV/VEPz3O/roym5Vs2PRzU/f32P0jL+//7DUHdwq88l5bHdr2cVXjh92zw36BMOrOLRELJLhIJJbtIJNRnl3Lkh6Em6KO+cO/b0vIfnbclqHtf33fT8t6xrC9+3MPhu67cOet3B3YHdfvGszgW92Z9+0eO9QbHDfVkQ2rvnnEkqLvxnE1pufLh8Py494NZXF/9ySVp+byPPB4cF/wbTDD8WCad2UUioWQXiYT5BA8YtNpsm+cX21Vtez+ZfnZ867eD7Y++5cdp+cnXhoK645Wsl9lF9nc6pzdsZp8745W0fKQSzo52bl9Wt288a3IPdoUPwozmugaHxmcEdVsP/xaNeOvgr9Lyvz1zeVB31h8+1dDPaNZG38BBf9Vq1enMLhIJJbtIJJTsIpHQ0JuU7vD1F6flj77le0HdlkPnpOViX/zoeNb/rpB1Qw+OhX3qR/YvTss9XeEw1s6+uWn5nP5X0/KvC/3yR189Ny0PdIdPts3rC+OqJ9+3/8TS9UHdHTddn5bPvLP+5Bhl0pldJBJKdpFIqBkvpdu1vP7cbIM92RDYgdGZdeu6ySab6CK8+21wIDuu18JmfH9X1iTfOTInLb86Ohgcd85gNkFFfsivaEZ3+Lvk3y/fvXhl7LTguNOXZ8Ny3Fn3x5dKZ3aRSCjZRSKhZryU7m8vzB5o2X5sflB34azn0/KjBxcHdfmm+0glbLrn5ZvdY4QPmeSb2a+NZw/C9HWFV9zDbkJ4V2n+Cn/Fw5vTDuau6r+u77W0XOxOfGzRj9LynZxX47con87sIpFQsotEQskuEgn12aUUPQuzO+MuHHg0LT926NzguPGBrA/8tlkvBXWbDi5Ky/P6Dqflw2PhmoH5/vdYJeyz55+Cm9GdTSpZ7FPn++LFu/Dy/fnxwvnx6Hh2LWFRf/aEXXHo7YkjC9OyX7YsqLNHttAODZ/Zk2WbnzCzdcn2IjPbaGbbzexeM+ub7GeISOecTDP+ZmBbbvvzwBfcfTGwH1jZysBEpLUaasab2RDw+8A/Ah8zMwOuBP4kOWQN8BngSyXEKKegQ8uy+dXXHcrmmRv3mvMqAP9/QomRXJM838zu7xoNjsvfJTdqYTN+tDBfXa2fB+Gc8nil4bp89+Lcvj1p+fmRcImqmV3ZnXf7zwsfwpn3SM0QW67RM/sXgU9C2nl5HXDA3U/8K+8AzmptaCLSSpMmu5m9H9jj7o9Pdmyd168ys81mtnmUkclfICKlaKQZfxnwATO7FhgAZgN3AHPMrCc5uw8BO2u92N1XA6uhOgddS6IWkZPWyPrstwG3AZjZFcAn3P1DZvYN4HrgHmAFsLa8MOVUs/P6rF/9+p5DaXlX9+nBcd25/nB+iAvg4PGsb3v+YDYf/L7CE2v5PnW/FZZUDn9kzddMVjfUlz0Rt2Hv+UHdLUPhJBUnvHI8HHpbPCPrz+97d/jk3Lwv1w2lpZq5qeZTVC/Wbafah7+rNSGJSBlO6qYad38IeCgpPwdc1PqQRKQMuoNOSjFr1rG0vDd3N9kVs38eHHfdYPak2JPHjwV1ZwxkdSOe/akeLcwN3194gi2vXnO92GXozd01d2S8/v1hM3vCJvj/js5Ly382e29aHp23KThuNBf/aXMam9Ou1XRvvEgklOwikdDyT9JRI++7MC3/6p1hr/Kf/3hNWr5/3++k5Xm9h4PjerqyJvlED7hMZKKr87tHZqflq+duDeo+89UPpeUzn8hGIAaf3hMcN/b8iw3F0Swt/yQiSnaRWCjZRSKhoTfpqP7vZ0NUi74f1l344azfe3/+NYWhtvyEEo320Yvyr5vZXX+e+92jc4Lt7tzjHv3fzX6X+oOBnaMzu0gklOwikVAzXsphdZrTVji/VLKhsu454UMyR3KjYePB5BVhI3lkguWa8kNq3VbnqRhg3Ouf97pyrztSuHtvvL94dPKawfBhHR/J2vs+Hg4P0qbhb53ZRSKhZBeJhJJdJBLqs0v58n3SCUbG/Fg4bdm/7ntnWu4vzOU+FaOV2pNPFhX777Ny42vHCmvO9b5GTX48nBTTxzo/GKczu0gklOwikVAzXspRZzjJusJ2fH4a9sqxcPKKA4W55k5odKgNwuG2Sm4O+eJ88vnXFYfX8k/VVQr9kHrzZlhf2Nz30fp35bWLzuwikVCyi0RCzXhpK680frfYK8dmpeWFs/Y19JrigzC9ueZ5vqnehReOy672F39GvtvQ3x22223s1FkKQWd2kUgo2UUioWQXiYT67DJt5Ye5ihNJTkU4DFf/Vr7iU3UHx7JlqAYKy0WPD0xtsoxOaHR99heAQ8A4MObuw2Y2D7gXWAi8ANzg7vvr/QwR6ayTaca/x92Xuftwsn0rsMHdlwAbkm0RmaaaacYvB65IymuorgH3qSbjEUmNjGV/nvlm/HjhHFVcyimoyzXdg3vmCqe5/MMvxaZ6zwQP4Uww58W002ioDvzIzB43s1XJvvnuvispvwzMb3l0ItIyjZ7ZL3f3nWZ2JrDezILV+dzdzWovqZH857AKYICZTQUrIlPX0Jnd3Xcm3/dQndX3ImC3mS0ASL7vqfPa1e4+7O7DvdSZsEtESjdpspvZoJmddqIMvBfYCjwArEgOWwGsLStIiVMFS7/G6Uq/ynas0ht89dp4+jXq3cFX93HSr+mukWb8fOB+q84W2gP8h7v/wMw2AfeZ2UrgReCG8sIUkWZNmuzu/hzw9hr79wFaklXkFKE76GTayj+Zlh9eK6Mpn7+jbpT6E1sU7+Sb4mpTHXEKjRKKSDOU7CKRULKLREJ9dpm2eruz/nG+3zx+EpPD5G+DnWitt4nWhDs63ps7Lqwbqz0nZtvWbzsZOrOLRELJLhIJNeNl2urLTSIx0cQTxbni8/ITYOQH1IoTTuYH1IrDa0dz88hXCo+5HT2zdtegcvRo3Zg6RWd2kUgo2UUioWa8TFv5ZZeKze56ilfSp/K64iquE/2MytzRmvutL1xCykdGah7XTjqzi0RCyS4SCSW7SCTUZ5dpa7AnmxHitfFslqPicsv5J+KOFpZbDobvJnhyLj/cVhzK689NQFlcztkO10mhk1jTrl10ZheJhJJdJBJqxktbWVd495vnRsq6BsOnSpbO+lVaPq3rWFou3uF2xcztdd+vN/d2E53Z8gN2Dx1ZGNQdqWRdiIV9e4O6h5e+KS3nG+7WHb6b1x6hayud2UUioWQXiYSSXSQS6rNLW/kEQ1KVw4eD7X95OJu82MayzveMneHQ2+0TTfo4lRGwws/Lr+B8+I3h9YLTt2WxnMnOtFyZBrfHFunMLhIJJbtIJNSMl/aq1F/+uOi8v3isxEBKdqrOQWdmc8zsm2b2czPbZmaXmtk8M1tvZs8m3+eWHayITF2jzfg7gB+4+/lUl4LaBtwKbHD3JcCGZFtEpqlGVnE9HXgXcBeAux939wPAcmBNctga4LpyQhSRVmjkzL4IeAX4spk9YWb/nizdPN/ddyXHvEx1tVcRmaYaSfYe4B3Al9z9AuAwhSa7uzt1RjTNbJWZbTazzaNMv7FHkVg0kuw7gB3uvjHZ/ibV5N9tZgsAku97ar3Y3Ve7+7C7D/fSX+sQEWmDSZPd3V8GXjKzNye7rgKeBh4AViT7VgBrS4lQRFqi0XH2vwK+ZmZ9wHPAR6j+R3Gfma0EXgRuKCdEEWmFhpLd3bcAwzWqrqqxT0SmId0uKxIJJbtIJJTsIpFQsotEQskuEgklu0gklOwikTBv40P2ZvYK1RtwXg/sneTwsk2HGEBxFCmO0MnG8UZ3P6NWRVuTPX1Ts83uXusmnahiUByKo51xqBkvEgklu0gkOpXsqzv0vnnTIQZQHEWKI9SyODrSZxeR9lMzXiQSbU12M7vGzJ4xs+1m1rbZaM3sbjPbY2Zbc/vaPhW2mZ1tZg+a2dNm9pSZ3dyJWMxswMweM7OfJnF8Ntm/yMw2Jp/Pvcn8BaUzs+5kfsN1nYrDzF4ws5+Z2RYz25zs68TfSGnTtrct2c2sG7gTeB+wFLjRzJa26e2/AlxT2NeJqbDHgI+7+1LgEuCm5N+g3bGMAFe6+9uBZcA1ZnYJ8HngC+6+GNgPrCw5jhNupjo9+QmdiuM97r4sN9TVib+R8qZtd/e2fAGXAj/Mbd8G3NbG918IbM1tPwMsSMoLgGfaFUsuhrXA1Z2MBZgJ/AS4mOrNGz21Pq8S338o+QO+ElhHdVnFTsTxAvD6wr62fi7A6cDzJNfSWh1HO5vxZwEv5bZ3JPs6paNTYZvZQuACYGMnYkmazluoThS6HvglcMDdT6xZ2q7P54vAJ4FKsv26DsXhwI/M7HEzW5Xsa/fnUuq07bpAx8RTYZfBzGYB3wJucfeDnYjF3cfdfRnVM+tFwPllv2eRmb0f2OPuj7f7vWu43N3fQbWbeZOZvStf2abPpalp2yfTzmTfCZyd2x5K9nVKQ1Nht5qZ9VJN9K+5+7c7GQuAV1f3eZBqc3mOmZ2Yl7Adn89lwAfM7AXgHqpN+Ts6EAfuvjP5vge4n+p/gO3+XJqatn0y7Uz2TcCS5EprH/BBqtNRd0rbp8I2M6O6jNY2d7+9U7GY2RlmNicpz6B63WAb1aS/vl1xuPtt7j7k7gup/j38l7t/qN1xmNmgmZ12ogy8F9hKmz8XL3va9rIvfBQuNFwL/IJq//Bv2vi+Xwd2AaNU//dcSbVvuAF4FvhPYF4b4ricahPsSWBL8nVtu2MB3gY8kcSxFfh0sv9c4DFgO/ANoL+Nn9EVwLpOxJG830+Tr6dO/G126G9kGbA5+Wy+A8xtVRy6g04kErpAJxIJJbtIJJTsIpFQsotEQskuEgklu0gklOwikVCyi0Ti/wDf2KfC0yaHBwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(fashion_batch[0].cpu().detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11aabd430>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(rec_img[0].cpu().detach().numpy().squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('lifelong_disrep')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
