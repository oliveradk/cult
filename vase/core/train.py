# AUTOGENERATED! DO NOT EDIT! File to edit: 03a_core.train.ipynb (unless otherwise specified).

__all__ = ['rec_likelihood', 'reconstruction_loss', 'kl_div_stdnorm', 'kl_div_target']

# Cell
import torch
from torch import nn
from torch.nn import functional as F
from ..config import DATA_PATH, PARAM_PATH


# Cell
def rec_likelihood(x, x_rec):
    """Returns element wise reconstruction loss across batch"""
    return F.binary_cross_entropy(x_rec, x, reduction='none').flatten(start_dim=1).sum(dim=1)

# Cell
def reconstruction_loss(x, x_rec):
    """Returns mean reconstruction loss across batch"""
    return torch.mean(rec_likelihood(x, x_rec))

# Cell
def kl_div_stdnorm(mu, logvar):
    """Returns element wise KL Divergence across batch"""
    return .5 * torch.sum(1 + logvar - mu.pow(2) - logvar, dim=1)#torch.mean(0.5 * (logvar.exp() + mu.pow(2) - 1) - logvar) #NOTE: this might be off, other implementations scale logvar too #-.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()) #

# Cell
def kl_div_target(mu, logvar, C=0, gamma=1):
    """Returns target loss: squared difference of mean kldivergence and target C scaled by gamma"""
    return gamma * torch.mean(torch.abs((kl_div_stdnorm(mu, logvar) - C)))