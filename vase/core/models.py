# AUTOGENERATED! DO NOT EDIT! File to edit: 02a_core.models.ipynb (unless otherwise specified).

__all__ = ['Encoder', 'FCEncoder', 'EnvironmentInference', 'env_dist_to_one_hot', 'Decoder', 'FCDecoder', 'reparam',
           'VanillaVAE', 'PaperVanillaVAE', 'FCVAE']

# Cell
import torch
from torch import nn
from torch.nn import functional as F

from ..config import DATA_PATH

# Cell
class Encoder(nn.Module):
    def __init__(self, latents=10):
        super().__init__()
        self.latents = latents
        #NOTE: no pooling? should compare results with and without
        self.conv1 = nn.Conv2d(1, 64, (4,4), stride=2, padding=1)
        self.conv2 = nn.Conv2d(64, 64, (4,4), 2, padding=1)
        self.conv3 = nn.Conv2d(64, 128, (4,4), 2, padding=1)
        self.conv4 = nn.Conv2d(128, 128, (4,4), 2, padding=1)
        self.linear = nn.Linear(2048, 256)
        self.linear_mu = nn.Linear(256, self.latents)
        self.linear_logvar = nn.Linear(256, self.latents)
        self.relu = nn.ReLU()

    def forward(self, x):
        """
        Returns mean and standard deviation to parameterize sigmoid,
        and final layer to compute environment
        """
        x = self.relu(self.conv1(x)) # (batch_size, 64, 32, 32)
        x = self.relu(self.conv2(x)) # (batch_size, 64, 16, 16)
        x = self.relu(self.conv3(x)) # (batch_size, 128, 8, 8)
        x = self.relu(self.conv4(x)) # (batch_size, 128, 4, 4)
        x = x.reshape(-1, 2048)
        final = self.relu(self.linear(x))
        mu = self.linear_mu(final)
        logvar = self.linear_logvar(final)
        return mu, logvar, final.detach() #detach to prevent gradient flow

# Cell
class FCEncoder(nn.Module):
    def __init__(self, latents: int):
        super().__init__()
        self.latents = latents
        self.latents = latents
        self.linear1 = nn.Linear(784, 50)
        self.linear_mu = nn.Linear(50, latents)
        self.linear_logvar = nn.Linear(50, latents)
        self.act = nn.ReLU()

    def forward(self, x):
        x = x.reshape(-1, 784)
        final = self.act(self.linear1(x))
        mu = self.linear_mu(final)
        logvar = self.linear_logvar(final) #TODO: should this be exponentiated?
        return mu, logvar, final

# Cell
class EnvironmentInference(nn.Module):
    def __init__(self, max_environmnets: int, input_dim:int):
        super().__init__()
        self.max_environments = max_environmnets
        self.input_dim = input_dim
        self.linear = nn.Linear(input_dim, max_environmnets)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, final_latent):
        x = self.linear(final_latent)
        return self.softmax(x)

# Cell
def env_dist_to_one_hot(env_dist: torch.Tensor, max_environments: int) -> torch.Tensor:
    """Converts a batch of distributions to a one-hot vector"""
    avg_env_dist = env_dist.mean(dim=0)
    env_idx = torch.argmax(avg_env_dist)
    return F.one_hot(env_idx, num_classes=max_environments)


# Cell
class Decoder(nn.Module):
    def __init__(self, latents:int, max_envs=0):
        super().__init__()
        self.max_envs = max_envs
        self.latents = latents
        self.linear2 = nn.Linear(latents + max_envs, 256)
        self.linear1 = nn.Linear(256, 2048)
        self.conv4 = nn.ConvTranspose2d(128, 128, (4,4), 2, padding=1)
        self.conv3 = nn.ConvTranspose2d(128, 64, (4,4), 2, padding=1)
        self.conv2 = nn.ConvTranspose2d(64, 64, (4,4), 2, padding=1)
        self.conv1 = nn.ConvTranspose2d(64, 1, (4,4), 2, padding=1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, z, s=None):
        """
        Decode the latent and environmental variables

        Args:
            z (Tensor): latent variables
            s (Tesnor): one-hot encoded environmental variable (not sure how this works...)

        Returns:
            Means for (batchsize, widgt, height) Bernoulli's (which can be interpreted as the reconstructed image)
        """
        batch_size = z.shape[0]
        if s is not None:
            s = s.expand(batch_size, -1)
            z = torch.cat((z, s), dim=1)
        x = self.relu(self.linear2(z)) # (batch_size, 256)
        x = self.relu(self.linear1(x)) # (batch_size, 512)
        x = x.reshape(-1, 128, 4, 4) # (batch_size, 128, 2, 2)
        x = self.relu(self.conv4(x)) # (batch_size, 128, 6, 6)
        x = self.relu(self.conv3(x)) # (batch_size, 64, 14, 14)
        x = self.relu(self.conv2(x)) # (batch_size, 64, 30, 30) WRONG (should be 31)
        out = self.sigmoid(self.conv1(x))
        return out

# Cell
class FCDecoder(nn.Module):
    def __init__(self, latents: int, max_envs=0):
        super().__init__()
        self.max_envs = max_envs
        self.latents = latents
        self.linear1 = nn.Linear(latents + max_envs, 50)
        self.linear2 = nn.Linear(50, 784)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, z, s=None):
        """
        Decode the latent and environmental variables

        Args:
            z (Tensor): latent variables
            s (Tesnor): one-hot encoded environmental variable (not sure how this works...)

        Returns:
            Means for (batchsize, widgt, height) Bernoulli's (which can be interpreted as the reconstructed image)
        """
        batch_size = z.shape[0]
        if s is not None:
            s = s.expand(batch_size, -1)
            z = torch.cat((z, s), dim=1)
        x = self.relu(self.linear1(z))
        x = self.linear2(x)
        out = self.sigmoid(x)
        out = out.reshape(-1, 1, 28, 28)
        return out

# Cell
def reparam(mu, logvar):
    eps = torch.randn(logvar.shape)
    std = (0.5 * logvar).exp()
    return mu + std * eps

# Cell
class VanillaVAE(nn.Module):
    def __init__(self, encoder: type, decoder: type, latents: int):
        super().__init__()
        self.encoder = encoder(latents=latents)
        self.decoder = decoder(latents=latents)

    def forward(self, x):
        mu, logvar, _final = self.encoder(x)
        if self.training:
            z = reparam(mu, logvar)
        else:
            z = mu
        rec_img = self.decoder(z=z)
        return rec_img, mu, logvar

# Cell
class PaperVanillaVAE(VanillaVAE):
    def __init__(self, latents: int):
        super().__init__(encoder=Encoder, decoder=Decoder, latents=latents)

# Cell
class FCVAE(VanillaVAE):
    def __init__(self, latents: int):
        super().__init__(encoder=FCEncoder, decoder=FCDecoder, latents=latents)
