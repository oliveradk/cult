{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core.vase_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vase Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model:\n",
    "    #args: ()\n",
    "\n",
    "#components:\n",
    "    #encoder/decoder\n",
    "        #latents\n",
    "    #environment inference network\n",
    "        #network that takes last hidden dim of encoder, outputs distribution over max environments\n",
    "        #max environments\n",
    "        #optimizer\n",
    "        #m current environment\n",
    "    #generative replay\n",
    "        #old_encoder, old_decoder\n",
    "        #tau #number of training steps per update - if none update on each new environment\n",
    "        #batch size of samples (64)\n",
    "    #latent masks\n",
    "        #store latent mask for each enviornment\n",
    "        #lamda, lamda1, lambda2\n",
    "    \n",
    "    #used_components\n",
    "        #optimizer iterations\n",
    "    #device\n",
    "#class VASE(nn.Module):\n",
    "#   #def __init__(self, \n",
    "#       encoder_type: type,\n",
    "#       decoder_type: type,\n",
    "#       latents: int,\n",
    "#       final_size: int,\n",
    "#       max_envs: int, \n",
    "#       \n",
    "#         \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #def forward(self, x):\n",
    "        #batch_size = x.shape[0]\n",
    "        #mu, logvar, final = self.encoder(x)\n",
    "        #s_hat, env_logits = self.get_likely_env(final)\n",
    "        #if self.training:\n",
    "            #z = self.reparam(mu, logvar)\n",
    "        \n",
    "        #a, alphas = self.get_mask(z)\n",
    "        #masked_z, masked_mu, masked_logvar = self.apply_mask(z, a, mu, logvar)\n",
    "\n",
    "        #s = self.infer_env(x, batch_size, masked_z, s_hat, a)\n",
    "        \n",
    "        #rec_x = self.decoder(masked_z, self.int_to_vec(s, batch_size))\n",
    "\n",
    "        #x_halu, s_halu = self.sample_old(s)\n",
    "        #with torch.no_grad():\n",
    "            #mu_halu_old, #logvar_halu_old, #final_halu_old = self.old_encoder(x_halu)\n",
    "            #z_halu_old = self.reparam(mu_halu_old, mu_logvar_old)\n",
    "            #a_halu_old = torch.tensor([self.masks[s] for s in list(s_halu)])?\n",
    "            #masked_z_halu_old, masked_mu_halu_old, masked_logvar_halu_old = self.apply_mask(z_halu_old, a_halu_old)\n",
    "        \n",
    "        #mu_halu, logvar_halu, final_halu = self.encoder(x_halu)\n",
    "        #z_halu = self.reparam(mu_halu, logvar_halu)\n",
    "        #a_halu, alphas_halu = self.get_mask(z_halu)\n",
    "        #masked_z_halu, masked_mu_halu, masked_logvar_halu = self.apply_mask(z_halu, a_halu, logvar_halu)\n",
    "        #rec_x_halo = self.decoder(masked_z_halu, s_halu)\n",
    "\n",
    "        #self.train_env_network(env_logits, s, final_halu_old, s_halu)\n",
    "\n",
    "        #return rec_x, masked_mu, masked_logvar, x_halu, masked_z_halu_old, rec_x_halo, masked_z_halu\n",
    "\n",
    "    #def get_likely_env(self, final):\n",
    "        #env_logits = self.env_network(final)\n",
    "        #avg_env_logits = torch.mean(env_logits, dim=0)\n",
    "        #return torch.argmax(avg_env_logits), env_logits\n",
    "    \n",
    "    #def reparam(self, mu, logvar):\n",
    "        # eps = torch.randn(logvar.shape).to(self.devicedevice)\n",
    "        # std = (0.5 * logvar).exp()\n",
    "        # return mu + std * eps\n",
    "    \n",
    "    #def infer_env(self, x, batch_size, masked_z, s_hat, a):\n",
    "        #with torch.no_grad():\n",
    "            #rec_x = self.decoder(masked_z, self.int_to_vec(s_hat, batch_size))\n",
    "        #rec_loss = torch.mean(rec_likelihood(x, rec_x))\n",
    "        \n",
    "        #if m == 0:\n",
    "            #self.init_env(rec_loss, a)\n",
    "            #return 0\n",
    "        \n",
    "        #if self.m == self.max_envs - 1:\n",
    "            #self.update_env(rec_loss, a, self.m)\n",
    "            #return self.m\n",
    "        \n",
    "        #if rec_loss > self.tau * self.rec_losses[s_hat]:\n",
    "            #self.m+=1\n",
    "            #self.init_env(rec_x, a)\n",
    "            #return self.m\n",
    "        \n",
    "        #if a != self.masks[s_hat]:\n",
    "            #u = self.get_used_mask(s_hat) \n",
    "            #if a * u != self.masks[s_hat] * u:\n",
    "            #   self.m+=1\n",
    "            #   self.init_env(rec_x, a)\n",
    "            #   return self.m\n",
    "\n",
    "        #self.update_env(rec_loss, a, s_hat)\n",
    "        #return s_hat \n",
    "        \n",
    "    #def init_env(self, rec_x, a):\n",
    "        #self.rec_losses.append(x)\n",
    "        #self.masks.append(a)\n",
    "    \n",
    "    #def update_env(self, rec_loss, a, s):\n",
    "        #self.rec_losses[s] = rec_loss\n",
    "        #self.masks[s] = a\n",
    "    \n",
    "    #def get_used_mask(x, batch_size, z, s_hat):\n",
    "        # optimizer = CMA(mean=np.zeros(self.latents), sigma=self.used_sigma)\n",
    "        # s_vec = self.int_to_vec(s_hat, batch_size)\n",
    "        # for generation in range(epochs):\n",
    "        #     solutions = []\n",
    "        #     for _ in range(optimizer.population_size):\n",
    "        #         sigma_a = optimizer.ask()\n",
    "        #         with torch.no_grad():\n",
    "        #             sigma_t = torch.tensor(x, dtype=torch.float)\n",
    "        #             sigma_t = torch.abs(sigma)\n",
    "        #             eps = torch.randn(sigma_t.shape[0]) * sigma_t\n",
    "        #             z_e = (1-delta) * z + (delta + eps)\n",
    "        #             rec_batch = self.decoder(z_e, s_vec)\n",
    "        #             rec_loss = torch.mean(rec_likelihood(batch, rec_batch))\n",
    "        #             sum_sigma = torch.sum(sigma_t)\n",
    "        #             loss = rec_loss - sum_sigma\n",
    "        #         value = loss.detach().numpy()\n",
    "        #         solutions.append((sigma_a, value))\n",
    "        #     optimizer.tell(solutions)\n",
    "        # sigma_p_a = sorted(solutions, key=lambda pair: pair[1], reverse=True)[0][0]\n",
    "        # sigma_p_t = torch.tensor(sigma_p_a, dtype=torch.float).abs()\n",
    "        # return sigma_prime_t < self.Tau\n",
    "\n",
    "    \n",
    "    #def int_to_vec(self, i, size):\n",
    "        #return torch.ones([size], dtype=torch.int64) * i\n",
    "    \n",
    "    #def apply_mask(z, a)\n",
    "        #TODO: verify it broadcasts correctly in both cases\n",
    "        #std_norm = torch.randn(z.shape)\n",
    "        # masked_z = a * z + (~a) * std_norm # \"reparam\" trick again\n",
    "        #masked_mu = mu * a\n",
    "        #masked_logvar = logvar * a\n",
    "        # return masked_z, masked_mu, masked_logvar\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('lifelong_disrep')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
