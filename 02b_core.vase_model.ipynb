{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core.vase_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "from cmaes import CMA\n",
    "import numpy as np\n",
    "\n",
    "from vase.core.models import FCEncoder, FCDecoder, Encoder, Decoder, EnvironmentInference\n",
    "from vase.core.utils import rec_likelihood, disable_gradient, kl_div_stdnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vase Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VASE(nn.Module):\n",
    "    def __init__(self, \n",
    "        encoder_type: type,\n",
    "        decoder_type: type,\n",
    "        latents: int,\n",
    "        final_size: int,\n",
    "        max_envs: int,\n",
    "        env_optim: type, \n",
    "        env_lr: float,\n",
    "        replay_batch_size: int,\n",
    "        tau: int,\n",
    "        kappa: float,  \n",
    "        lam: float,\n",
    "        lam_1: float,\n",
    "        lam_2: float, \n",
    "        Tau: float,\n",
    "        used_epochs: int,\n",
    "        used_delta: float,\n",
    "        used_sigma: float,\n",
    "        device: str\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.latents = latents\n",
    "        self.max_envs = max_envs\n",
    "        self.final_size = final_size\n",
    "        self.device = device\n",
    "        self.encoder = encoder_type(self.latents)\n",
    "        self.decoder = decoder_type(self.latents, self.max_envs)\n",
    "        self.old_encoder = encoder_type(self.latents)\n",
    "        self.old_decoder = decoder_type(self.latents, self.max_envs)\n",
    "        self.copy_and_freeze()\n",
    "        self.encoder.to(self.device), self.decoder.to(self.device), self.old_encoder.to(self.device), self.old_decoder.to(self.device)\n",
    "        self.steps = 0\n",
    "        self.tau = tau\n",
    "        self.replay_batch_size = replay_batch_size\n",
    "        self.env_net = EnvironmentInference(self.max_envs, self.final_size)\n",
    "        self.m = 0\n",
    "        self.env_optim = env_optim(params=self.env_net.parameters(), lr=env_lr)\n",
    "        self.env_loss = nn.CrossEntropyLoss()\n",
    "        self.rec_losses = []\n",
    "        self.kappa = kappa\n",
    "        self.masks = []\n",
    "        self.lam = lam\n",
    "        self.lam_1 = lam_1\n",
    "        self.lam_2 = lam_2\n",
    "        self.Tau = Tau\n",
    "        self.used_epochs = used_epochs\n",
    "        self.used_delta = used_delta\n",
    "        self.used_sigma = used_sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        self.steps += batch_size\n",
    "\n",
    "        mu, logvar, final = self.encoder(x)\n",
    "        s_hat, env_logits = self.get_likely_env(final)\n",
    "        if self.training:\n",
    "            z = self.reparam(mu, logvar)\n",
    "        \n",
    "        a, alphas = self.get_mask(z)\n",
    "        masked_z, masked_mu, masked_logvar = self.apply_mask(z, a, mu, logvar)\n",
    "\n",
    "        s = self.infer_env(x, batch_size, masked_z, s_hat, a)\n",
    "        \n",
    "        rec_x = self.decoder(masked_z, self.int_to_vec(s, batch_size))\n",
    "\n",
    "        x_halu, s_halu = self.sample_old(s)\n",
    "        with torch.no_grad():\n",
    "            mu_halu_old, logvar_halu_old, final_halu_old = self.old_encoder(x_halu)\n",
    "            z_halu_old = self.reparam(mu_halu_old, logvar_halu_old)\n",
    "            a_halu_old = torch.tensor([self.masks[s] for s in list(s_halu)])\n",
    "            masked_z_halu_old, masked_mu_halu_old, masked_logvar_halu_old = self.apply_mask(z_halu_old, a_halu_old)\n",
    "        \n",
    "        mu_halu, logvar_halu, final_halu = self.encoder(x_halu)\n",
    "        z_halu = self.reparam(mu_halu, logvar_halu)\n",
    "        a_halu, alphas_halu = self.get_mask(z_halu)\n",
    "        masked_z_halu, masked_mu_halu, masked_logvar_halu = self.apply_mask(z_halu, a_halu, mu_halu, logvar_halu)\n",
    "        rec_x_halo = self.decoder(masked_z_halu, s_halu)\n",
    "\n",
    "        self.train_env_network(env_logits, s, final_halu_old, s_halu)\n",
    "\n",
    "        if self.tau and self.steps > self.tau:\n",
    "            self.copy_and_freeze()\n",
    "\n",
    "        return rec_x, masked_mu, masked_logvar, x_halu, masked_z_halu_old, rec_x_halo, masked_z_halu\n",
    "\n",
    "    def get_mask(self, z):\n",
    "        std, mean = torch.std_mean(z, dim=0)\n",
    "        std = std[:,None]\n",
    "        mean = mean[:, None]\n",
    "        logvar = torch.log(std.pow(2))\n",
    "        alphas = kl_div_stdnorm(mean, logvar)\n",
    "        alphas[alphas < self.lam_1] = 0\n",
    "        alphas[alphas > self.lam_2] = 1\n",
    "        a = alphas < self.lam\n",
    "        return a, alphas\n",
    "    \n",
    "    def get_likely_env(self, final):\n",
    "        env_logits = self.env_network(final)\n",
    "        avg_env_logits = torch.mean(env_logits, dim=0)\n",
    "        return torch.argmax(avg_env_logits), env_logits\n",
    "    \n",
    "    def reparam(self, mu, logvar):\n",
    "        eps = torch.randn(logvar.shape).to(self.devicedevice)\n",
    "        std = (0.5 * logvar).exp()\n",
    "        return mu + std * eps\n",
    "    \n",
    "    def infer_env(self, x, batch_size, masked_z, s_hat, a):\n",
    "        with torch.no_grad():\n",
    "            rec_x = self.decoder(masked_z, self.int_to_vec(s_hat, batch_size))\n",
    "        rec_loss = torch.mean(rec_likelihood(x, rec_x))\n",
    "        \n",
    "        if self.m == 0:\n",
    "            self.init_env(rec_loss, a)\n",
    "            return 0\n",
    "        \n",
    "        if self.m == self.max_envs - 1:\n",
    "            self.update_env(rec_loss, a, self.m)\n",
    "            return self.m\n",
    "        \n",
    "        if rec_loss > self.kappa * self.rec_losses[s_hat]:\n",
    "            self.m+=1\n",
    "            self.init_env(rec_x, a)\n",
    "            return self.m\n",
    "        \n",
    "        if a != self.masks[s_hat]:\n",
    "            u = self.get_used_mask(s_hat) \n",
    "            if a * u != self.masks[s_hat] * u:\n",
    "              self.m+=1\n",
    "              self.init_env(rec_x, a)\n",
    "              return self.m\n",
    "\n",
    "        self.update_env(rec_loss, a, s_hat)\n",
    "        return s_hat \n",
    "        \n",
    "    def init_env(self, rec_x, a):\n",
    "        if self.tau is None:\n",
    "            self.copy_and_freeze()\n",
    "        self.rec_losses.append(rec_x)\n",
    "        self.masks.append(a)\n",
    "    \n",
    "    def update_env(self, rec_loss, a, s):\n",
    "        self.rec_losses[s] = rec_loss\n",
    "        self.masks[s] = a\n",
    "    \n",
    "    def get_used_mask(self, x, batch_size, z, s_hat):\n",
    "        optimizer = CMA(mean=np.zeros(self.latents), sigma=self.used_sigma)\n",
    "        s_vec = self.int_to_vec(s_hat, batch_size)\n",
    "        for generation in range(self.used_epochs):\n",
    "            solutions = []\n",
    "            for _ in range(optimizer.population_size):\n",
    "                sigma_a = optimizer.ask()\n",
    "                with torch.no_grad():\n",
    "                    sigma_t = torch.tensor(x, dtype=torch.float)\n",
    "                    sigma_t = torch.abs(sigma_t)\n",
    "                    eps = torch.randn(sigma_t.shape[0]) * sigma_t\n",
    "                    z_e = (1-self.used_delta) * z + (self.used_delta + eps)\n",
    "                    rec_x = self.decoder(z_e, s_vec)\n",
    "                    rec_loss = torch.mean(rec_likelihood(x, rec_x))\n",
    "                    sum_sigma = torch.sum(sigma_t)\n",
    "                    loss = rec_loss - sum_sigma\n",
    "                value = loss.detach().numpy()\n",
    "                solutions.append((sigma_a, value))\n",
    "            optimizer.tell(solutions)\n",
    "        sigma_p_a = sorted(solutions, key=lambda pair: pair[1], reverse=True)[0][0]\n",
    "        sigma_p_t = torch.tensor(sigma_p_a, dtype=torch.float).abs()\n",
    "        return sigma_p_t < self.Tau\n",
    "\n",
    "    \n",
    "    def int_to_vec(self, i, size):\n",
    "        return torch.ones([size], dtype=torch.int64) * i\n",
    "    \n",
    "    def apply_mask(z, a, mu, logvar):\n",
    "        #TODO: verify it broadcasts correctly in both cases\n",
    "        std_norm = torch.randn(z.shape)\n",
    "        masked_z = a * z + (~a) * std_norm # \"reparam\" trick again\n",
    "        masked_mu = mu * a\n",
    "        masked_logvar = logvar * a\n",
    "        return masked_z, masked_mu, masked_logvar\n",
    "    \n",
    "    def copy_model(self, old_model, cur_model):\n",
    "        old_model.load_state_dict(cur_model.state_dict())\n",
    "    \n",
    "    def freeze_model(self, model):\n",
    "        disable_gradient(model)\n",
    "    \n",
    "    def copy_and_freeze(self):\n",
    "        self.copy_model(self.old_encoder, self.encoder)\n",
    "        self.freeze_model(self.old_encoder)\n",
    "        self.copy_model(self.old_decoder, self.decoder)\n",
    "        self.freeze_model(self.old_decoder)\n",
    "    \n",
    "    def train_env_network(self, env_logits, s, final_halu_old, s_halu):\n",
    "        self.env_optim.zero_grad()\n",
    "        cur_loss = self.env_loss(env_logits, s) #don't know if dims work here\n",
    "        env_logits_halu = self.env_net(final_halu_old)\n",
    "        replay_loss = self.env_loss(env_logits_halu, s_halu)\n",
    "        loss = cur_loss + replay_loss\n",
    "        loss.backward()\n",
    "        self.env_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents=8\n",
    "final_size=50\n",
    "max_envs=7\n",
    "env_optim=torch.optim.Adam\n",
    "env_lr=6e-4\n",
    "replay_batch_size=64\n",
    "tau=None\n",
    "kappa=1.5\n",
    "lam=1.5,\n",
    "lam_1=.01, \n",
    "lam_2=3, \n",
    "Tau=None, \n",
    "used_epochs=50 \n",
    "used_delta=.95\n",
    "used_sigma = 1.3\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "vase = VASE(FCEncoder, FCDecoder, latents, final_size, max_envs, env_optim, env_lr, replay_batch_size, tau, kappa, lam, lam_1, lam_2, Tau, used_epochs, used_delta, used_sigma, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('lifelong_disrep')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
