# AUTOGENERATED! DO NOT EDIT! File to edit: 02a_core.models.ipynb (unless otherwise specified).

__all__ = ['Encoder', 'FCEncoder', 'CNNEncoder', 'EnvironmentInference', 'env_dist_to_idx', 'Decoder', 'FCDecoder',
           'CNNDecoder', 'reparam', 'VanillaVAE', 'CNNVanillaVAE', 'FCVAE']

# Cell
import torch
from torch import nn
from torch.nn import functional as F
import numpy as np
import copy
from ..config import DATA_PATH
from .utils import rec_likelihood, kl_div_stdnorm, disable_gradient, enable_gradient

# Cell
class Encoder(nn.Module):
    def __init__(self, latents=10, device='cpu'):
        super().__init__()
        self.latents = latents
        self.conv1 = nn.Conv2d(1, 64, (4,4), stride=2, padding=1)
        self.conv2 = nn.Conv2d(64, 64, (4,4), 2, padding=1)
        self.conv3 = nn.Conv2d(64, 128, (4,4), 2, padding=1)
        self.conv4 = nn.Conv2d(128, 128, (4,4), 2, padding=1)
        self.linear = nn.Linear(2048, 256)
        self.linear_mu = nn.Linear(256, self.latents)
        self.linear_logvar = nn.Linear(256, self.latents)
        self.relu = nn.ReLU()
        self.device = device

    def forward(self, x):
        """
        Returns mean and standard deviation to parameterize sigmoid,
        and final layer to compute environment
        """
        x = self.relu(self.conv1(x)) # (batch_size, 64, 32, 32)
        x = self.relu(self.conv2(x)) # (batch_size, 64, 16, 16)
        x = self.relu(self.conv3(x)) # (batch_size, 128, 8, 8)
        x = self.relu(self.conv4(x)) # (batch_size, 128, 4, 4)
        x = x.reshape(-1, 2048)
        final = self.relu(self.linear(x))
        mu = self.linear_mu(final)
        logvar = self.linear_logvar(final)
        return mu, logvar, final.detach() #detach to prevent gradient flow

# Cell
class FCEncoder(nn.Module):
    def __init__(self, latents: int, device='cpu'):
        super().__init__()
        self.latents = latents
        self.latents = latents
        self.linear1 = nn.Linear(784, 50)
        self.linear_mu = nn.Linear(50, latents)
        self.linear_logvar = nn.Linear(50, latents)
        self.act = nn.ReLU()
        self.device = device

    def forward(self, x):
        x = x.reshape(-1, 784)
        final = self.act(self.linear1(x))
        mu = self.linear_mu(final)
        logvar = self.linear_logvar(final) #TODO: should this be exponentiated?
        return mu, logvar, final

# Cell
class CNNEncoder(nn.Module):
    def __init__(self, latents: int, device='cpu'):
        super().__init__()
        self.latents = latents
        self.conv1 = nn.Conv2d(1, 32, (4,4), stride=2, padding=1)
        self.conv2 = nn.Conv2d(32, 32, (4,4), 2, padding=0)
        self.conv3 = nn.Conv2d(32, 64, (4,4), 2, padding=0)
        self.flatten = nn.Flatten()
        self.linear1 = nn.Linear(256, 64)
        self.linear_mu = nn.Linear(64, latents)
        self.linear_logvar = nn.Linear(64, latents)
        self.act = nn.ReLU()
        self.device = device

    def forward(self, x):
        x = self.act(self.conv1(x))
        x = self.act(self.conv2(x))
        x = self.act(self.conv3(x))
        #x = self.act(self.conv4(x))
        x = self.flatten(x)
        final = self.act(self.linear1(x))
        mu = self.linear_mu(final)
        logvar = self.linear_logvar(final) #TODO: should this be exponentiated?
        return mu, logvar, final

# Cell
class EnvironmentInference(nn.Module):
    def __init__(self, max_environmnets: int, input_dim:int, hidden_dim=50):
        super().__init__()
        self.max_environments = max_environmnets
        self.input_dim = input_dim
        self.linear = nn.Linear(input_dim, self.max_environments)
        self.dropout = nn.Dropout(p=.5)
        self.softmax = nn.Softmax(dim=1)
        self.relu = nn.ReLU()

    def forward(self, final_latent):
        x = self.linear(final_latent)
        x = self.dropout(x)
        return self.softmax(x)

# Cell
def env_dist_to_idx(env_dist: torch.Tensor, max_environments: int) -> torch.Tensor:
    """Converts a batch of distributions to a one-hot vector"""
    batch_size = env_dist.shape[0]
    avg_env_dist = env_dist.mean(dim=0)
    env_idx = torch.argmax(avg_env_dist)
    return torch.ones((batch_size), dtype=torch.int64) * env_idx


# Cell
class Decoder(nn.Module):
    def __init__(self, latents:int, max_envs=0, device='cpu'):
        super().__init__()
        self.max_envs = max_envs
        self.latents = latents
        self.linear2 = nn.Linear(latents + max_envs, 256)
        self.linear1 = nn.Linear(256, 2048)
        self.conv4 = nn.ConvTranspose2d(128, 128, (4,4), 2, padding=1)
        self.conv3 = nn.ConvTranspose2d(128, 64, (4,4), 2, padding=1)
        self.conv2 = nn.ConvTranspose2d(64, 64, (4,4), 2, padding=1)
        self.conv1 = nn.ConvTranspose2d(64, 1, (4,4), 2, padding=1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        self.device = device

    def forward(self, z, s=None):
        """
        Decode the latent and environmental variables

        Args:
            z (Tensor): latent variables
            s (Tensor): environment indicies (not one hot)

        Returns:
            Means for (batchsize, widgt, height) Bernoulli's (which can be interpreted as the reconstructed image)
        """

        if s is not None:
            s_one_hot = F.one_hot(s, num_classes=self.max_envs).to(self.device)
            z = torch.cat((z, s_one_hot), dim=1)
        x = self.relu(self.linear2(z)) # (batch_size, 256)
        x = self.relu(self.linear1(x)) # (batch_size, 512)
        x = x.reshape(-1, 128, 4, 4) # (batch_size, 128, 2, 2)
        x = self.relu(self.conv4(x)) # (batch_size, 128, 6, 6)
        x = self.relu(self.conv3(x)) # (batch_size, 64, 14, 14)
        x = self.relu(self.conv2(x)) # (batch_size, 64, 30, 30) WRONG (should be 31)
        out = self.sigmoid(self.conv1(x))
        return out

# Cell
class FCDecoder(nn.Module):
    def __init__(self, latents: int, max_envs=0, device='cpu'):
        super().__init__()
        self.max_envs = max_envs
        self.latents = latents
        self.linear1 = nn.Linear(latents + max_envs, 50)
        self.linear2 = nn.Linear(50, 784)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        self.device = device

    def forward(self, z, s=None):
        """
        Decode the latent and environmental variables

        Args:
            z (Tensor): latent variables
            s (Tensor): one-hot encoded environmental variable (not sure how this works...)

        Returns:
            Means for (batchsize, widgt, height) Bernoulli's (which can be interpreted as the reconstructed image)
        """
        if s is not None:
            s_one_hot = F.one_hot(s, num_classes=self.max_envs).to(self.device)
            z = torch.cat((z, s_one_hot), dim=1)
        x = self.relu(self.linear1(z))
        x = self.linear2(x)
        out = self.sigmoid(x)
        out = out.reshape(-1, 1, 28, 28)
        return out

# Cell
class CNNDecoder(nn.Module):
    def __init__(self, latents: int, max_envs=0, device='cpu'):
        super().__init__()
        self.max_envs = max_envs
        self.latents = latents
        self.linear1 = nn.Linear(latents + max_envs, 64)
        self.linear2 = nn.Linear(64, 256)
        self.conv1 = nn.ConvTranspose2d(64, 32, (4,4), 2)
        self.conv2 = nn.ConvTranspose2d(32, 32, (4,4), 2)
        self.conv3 = nn.ConvTranspose2d(32, 1, (4,4), 2, 1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        self.device = device

    def forward(self, z, s=None):
        """
        Decode the latent and environmental variables

        Args:
            z (Tensor): latent variables
            s (Tensor): one-hot encoded environmental variable (not sure how this works...)

        Returns:
            Means for (batchsize, widgt, height) Bernoulli's (which can be interpreted as the reconstructed image)
        """
        if s is not None:
            s_one_hot = F.one_hot(s, num_classes=self.max_envs).to(self.device)
            z = torch.cat((z, s_one_hot), dim=1)
        x = self.relu(self.linear1(z))
        x = self.relu(self.linear2(x))
        x = x.reshape(-1, 64, 2, 2)
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.conv3(x)
        out = self.sigmoid(x)
        return out

# Cell
def reparam(mu, logvar, device='cpu'):
    eps = torch.randn(logvar.shape).to(device)
    std = (0.5 * logvar).exp()
    return mu + std * eps

# Cell
class VanillaVAE(nn.Module):
    def __init__(self, encoder: type, decoder: type, latents: int, device: str):
        super().__init__()
        self.encoder = encoder(latents=latents)
        self.decoder = decoder(latents=latents)
        self.device = device

    def forward(self, x):
        mu, logvar, _final = self.encoder(x)
        if self.training:
            z = reparam(mu, logvar, device=self.device)
        else:
            z = mu
        rec_img = self.decoder(z=z)
        return rec_img, mu, logvar

# Cell
class CNNVanillaVAE(VanillaVAE):
    def __init__(self, latents: int, device:str):
        super().__init__(encoder=Encoder, decoder=Decoder, latents=latents, device=device)

# Cell
class FCVAE(VanillaVAE):
    def __init__(self, latents: int, device='cpu'):
        super().__init__(encoder=FCEncoder, decoder=FCDecoder, latents=latents, device=device)
