{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from vase.config import DATA_PATH\n",
    "from vase.core.models import PaperVanillaVAE, FCVAE\n",
    "from vase.core.datasets.moving_mnist import MovingFashionMNIST, CommonMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "batch_size = 128\n",
    "latents = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "mnist_data = CommonMNIST(DATA_PATH, transform=ToTensor(), download=True)\n",
    "fc_vae_helper = FCVAE(latents)\n",
    "mnist_loader = DataLoader(mnist_data, batch_size=batch_size, shuffle=True)\n",
    "mnist_batch, _ = iter(mnist_loader).next()\n",
    "rec, mu, logvar = fc_vae_helper(mnist_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction Likelihood\n",
    "We'll use Binary Cross Entropy Loss with $y$ the ground truth image $x$, and $p(y)$ the reconstructed image. In terms of log likelihood, I'm not really sure how this makes sense, but it seems to be how its done... (TODO figure this out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def rec_likelihood(x, x_rec):\n",
    "    \"\"\"Returns element wise reconstruction loss across batch\"\"\"\n",
    "    return F.binary_cross_entropy(x_rec, x, reduction='none').flatten(start_dim=1).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rec_likelihood(mnist_batch, rec).shape == torch.Size([batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence Std Norm\n",
    "Recall the definition of KL Divergence is the expected value under the reference distribution of the information ratio (or something like that):\n",
    "\n",
    "$$D_{KL}(q||p) = E_q[\\log{\\frac{q}{p}}] $$\n",
    "\n",
    "So in our case, with\n",
    "$$KL(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(z))$$\n",
    "we have \n",
    "\n",
    "$$KL(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(z)) = E_{q_{\\phi}(\\mathbf{z}|x)}[\\log{q_{\\phi}(\\mathbf{z}|\\mathbf{x})} - \\log{p(z)}]$$\n",
    "\n",
    "Note that both $q_{\\phi}(\\mathbf{z}|x))$ and $p(z)$ are diagonal gaussians. The KL divergence between diagonal gaussians can be [derived analytically](https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes), and is given by:\n",
    "\n",
    "$$  \\frac{1}{2} \\left[-(\\sum_{i}{\\log{\\sigma_i^2} + 1}) + \\sum_{i}(\\sigma_i^2) + \\sum_{i}{\\mu_i^{2}} \\right]$$\n",
    "\n",
    "TODO: see Kigma 2014 for real derivation - this one is off (or Kigma's is off, idk...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def kl_div_stdnorm(mu, logvar):\n",
    "    \"\"\"Returns element wise KL Divergence across batch\"\"\"\n",
    "    return .5 * torch.sum(-(1 + logvar) + logvar.exp() + mu.pow(2), dim=1)#torch.mean(0.5 * (logvar.exp() + mu.pow(2) - 1) - logvar) #NOTE: this might be off, other implementations scale logvar too #-.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert kl_div_stdnorm(mu, logvar).shape == torch.Size([batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_stdnorm(torch.Tensor([0]), torch.log(torch.Tensor([1]))) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mu_1$ = 2, $\\sigma_1^2$ = 4, then we would have\n",
    "$$KL(q, p) = \\log \\frac{1}{4} + \\frac{4 + (2-0)^2}{2} - \\frac{1}{2} = 4 - \\frac{1}{2} + log{\\frac{1}{4}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert kl_div_stdnorm(torch.Tensor([2]), torch.log(torch.Tensor([4]))) == 4 - .5 + torch.log(torch.Tensor([.25]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('lifelong_disrep')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
